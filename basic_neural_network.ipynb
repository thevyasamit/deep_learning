{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([2], dtype=torch.float64)\n",
    "print(t1.shape)\n",
    "print(t1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Why using double? \n",
    "When we initialize tensor, by default, they are set to int64 and since we will be calculating grad (differentition) etc we will\n",
    "be encountering floating point values so we type cast them to double which is float64.\n",
    "\n",
    "Why setting required_grad? \n",
    "Since we will be doing backpropogation we need to store gradients, so we can simply say that this tensors object will require gradient\n",
    "by setting the required_grad flag of the pytorch's tensor class. \n",
    "Note: By default, these flags are set to false assuming we don't need gradients.\n",
    "'''\n",
    "x1 = torch.Tensor([2.0]).double()  # we can also do torch.Tensor([2.0], dtype=torch.float64)\n",
    "x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()               \n",
    "x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               \n",
    "w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                \n",
    "w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  \n",
    "b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n) #this will use the tanh\n",
    "print(o)\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_in) -> None:\n",
    "        self.w = [random.uniform(-1,1) for _ in range(n_in)]\n",
    "        self.b = random.uniform(-1,1)\n",
    "        print(\"Inside neuron\", \"w is : \",self.w, \"|| b is: \",self.b)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        ans = sum(wi*xi for wi,xi in zip(self.w,x))+self.b\n",
    "        out = torch.tanh(torch.tensor(ans))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside neuron w is :  [-0.29777854260485825, -0.771021648993027] || b is:  0.3152509195153086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.9889)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= [2.0,3.0]\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self,n_in,n_out) -> None:\n",
    "        self.neurons = [Neuron(n_in) for _ in range(n_out)]\n",
    "        print(\"Inside Layer:  \", self.neurons)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        outputs = [n(x) for n in self.neurons]\n",
    "        return outputs[0] if len(outputs) == 1 else outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside neuron w is :  [-0.8575748629565829, 0.2201591427839924] || b is:  -0.9122106549918887\n",
      "Inside neuron w is :  [0.7687181738916289, 0.46774419945924595] || b is:  0.04989135551529933\n",
      "Inside neuron w is :  [0.8117356539180716, 0.60045575719421] || b is:  -0.44049945447606964\n",
      "Inside Layer:   [<__main__.Neuron object at 0x7fdef5333e50>, <__main__.Neuron object at 0x7fdeec9c6670>, <__main__.Neuron object at 0x7fdef4854fd0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(-0.9582), tensor(0.9958), tensor(0.9960)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0,3.2]\n",
    "n = Layer(2,3)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayer:\n",
    "    def __init__(self, n_in, n_outs) -> None:\n",
    "        size = [n_in]+n_outs\n",
    "        self.layers = [Layer(size[i],size[i+1]) for i in range(len(n_outs))]\n",
    "        print(\"inside MLP \", self.layers)\n",
    "    def __call__(self,x):\n",
    "        for layer in self.layers:\n",
    "            out = layer(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside neuron w is :  [-0.8046401897124831, -0.3341137105993741, 0.9515341252947482] || b is:  -0.00897120750212399\n",
      "Inside neuron w is :  [-0.5452189144602366, 0.006257188717469608, -0.029168079175063033] || b is:  -0.45325286660031305\n",
      "Inside neuron w is :  [0.5869830446784283, -0.26446117196514574, -0.9790511271638678] || b is:  -0.5647977815105776\n",
      "Inside neuron w is :  [0.47753499756376305, 0.15445036634329967, 0.24416044706729312] || b is:  -0.14656088519044697\n",
      "Inside Layer:   [<__main__.Neuron object at 0x7fdeebedf850>, <__main__.Neuron object at 0x7fdeebf6c9a0>, <__main__.Neuron object at 0x7fdef486d910>, <__main__.Neuron object at 0x7fdef486dbb0>]\n",
      "Inside neuron w is :  [-0.5215187412550524, 0.5647925951278503, -0.8111741715639671, -0.7232732188823427] || b is:  0.5544911241798507\n",
      "Inside neuron w is :  [-0.7045663107133848, 0.07185233988633088, 0.7719235142197287, -0.28545908959187516] || b is:  -0.8113344256960384\n",
      "Inside neuron w is :  [-0.3478687499309361, 0.182914969257858, -0.15388579558080773, -0.55198639473685] || b is:  0.8920859351435257\n",
      "Inside neuron w is :  [-0.6787313925000176, 0.12197904303784068, -0.7229870862085803, 0.9206221951913771] || b is:  0.9791243360931281\n",
      "Inside Layer:   [<__main__.Neuron object at 0x7fdef51b9d00>, <__main__.Neuron object at 0x7fdef51b9b20>, <__main__.Neuron object at 0x7fdef51b9f40>, <__main__.Neuron object at 0x7fdef51b9f70>]\n",
      "Inside neuron w is :  [-0.142298367553914, 0.616382517941733, 0.8559950948008175, 0.6484300505786122] || b is:  0.7451813620583532\n",
      "Inside Layer:   [<__main__.Neuron object at 0x7fdef51b9d90>]\n",
      "inside MLP  [<__main__.Layer object at 0x7fdeebedf4c0>, <__main__.Layer object at 0x7fdeebedf610>, <__main__.Layer object at 0x7fdef51b9940>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.9848)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.3,1.2,-4.2]\n",
    "n = MultiLayer(3,[4,4,1]) # here 3 is number of inputs layer not a hidden layer.\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
